{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":223675230,"sourceType":"kernelVersion"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U bitsandbytes\n!pip install -q -U datasets\n!pip install -q -U git+https://github.com/huggingface/transformers.git\n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install -q -U git+https://github.com/huggingface/accelerate.git\n!pip install -q -U loralib\n!pip install -q -U einops","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T15:20:31.739370Z","iopub.execute_input":"2025-02-21T15:20:31.739682Z","iopub.status.idle":"2025-02-21T15:21:46.066118Z","shell.execute_reply.started":"2025-02-21T15:20:31.739650Z","shell.execute_reply":"2025-02-21T15:21:46.065040Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for accelerate (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom transformers import (\n    AutoConfig, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n)\nfrom peft import (\n    LoraConfig, PeftConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n\nFINETUNED_MODEL = \"thangquang09/vinallama_math_solver_7B\"\n\nconfig = PeftConfig.from_pretrained(FINETUNED_MODEL)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    config.base_model_name_or_path,\n    return_dict=True,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\n\ntokenizer=AutoTokenizer.from_pretrained(config.base_model_name_or_path)\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = PeftModel.from_pretrained(model, FINETUNED_MODEL)\ngeneration_config = model.generation_config\ngeneration_config.max_new_tokens = 200\ngeneration_config.temperature = 0.7\ngeneration_config.top_p = 0.7\ngeneration_config.num_return_sequences = 1\ngeneration_config.pad_token_id = model.config.pad_token_id\ngeneration_config.eos_token_id = model.config.eos_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T16:17:34.669432Z","iopub.execute_input":"2025-02-21T16:17:34.669778Z","iopub.status.idle":"2025-02-21T16:17:51.474223Z","shell.execute_reply.started":"2025-02-21T16:17:34.669751Z","shell.execute_reply":"2025-02-21T16:17:51.473425Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b92800ebd0e475e8f7e7f47b3b3437d"}},"metadata":{}}],"execution_count":64},{"cell_type":"code","source":"IN_CONTEXT_PROMPT = {\n    \"2\": \"Bạn là một chuyên gia về toán học. Trả lời câu hỏi sau bằng cách đưa ra đáp án chính xác nhất. Đáp án sẽ là một trong các lựa chọn A, B, C, D. Hãy suy nghĩ từng bước một.\"\n    ,\"1\": \"Bạn là một chuyên gia về toán học. Trả lời câu hỏi sau bằng cách đưa ra đáp án chính xác nhất. Hãy suy nghĩ từng bước một.\"\n}\n\nPROMTP_ANS_FORMAT = \"\"\"\n<|im_start|>system\n{}\n<|im_end|>\n<|im_start|>user\n{}\n<|im_end|>\n<|im_start|>assistant\n\"\"\"\n\ndef make_ans_prompt(question, choices=None):\n    if choices is not None:\n        in_context_key = \"2\"\n        choices = f\"\"\"### Các lựa chọn\\n{choices}\n        \"\"\"\n    else:\n        in_context_key = \"1\"\n        choices = \"\"\n    \n    instruction = question + \"\\n\" + choices\n    instruction = instruction.strip()\n    \n    prompt = PROMTP_ANS_FORMAT.format(\n        IN_CONTEXT_PROMPT[in_context_key],\n        instruction\n    )\n\n    return prompt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T16:11:22.649768Z","iopub.execute_input":"2025-02-21T16:11:22.650088Z","iopub.status.idle":"2025-02-21T16:11:22.655123Z","shell.execute_reply.started":"2025-02-21T16:11:22.650064Z","shell.execute_reply":"2025-02-21T16:11:22.654305Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"print(make_prompt(\"1 nhân 2 bằng bao nhiêu?\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T15:55:32.841292Z","iopub.execute_input":"2025-02-21T15:55:32.841608Z","iopub.status.idle":"2025-02-21T15:55:32.846410Z","shell.execute_reply.started":"2025-02-21T15:55:32.841582Z","shell.execute_reply":"2025-02-21T15:55:32.845329Z"}},"outputs":[{"name":"stdout","text":"\n<|im_start|>system\nBạn là một chuyên gia về toán học. Trả lời câu hỏi sau bằng cách đưa ra đáp án chính xác nhất. Hãy suy nghĩ từng bước một.\n<|im_end|>\n<|im_start|>user\n1 nhân 2 bằng bao nhiêu?\n<|im_end|>\n<|im_start|>assistant\n\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"import re\n\ndef remove_duplicate_sentences(text):\n    sentences = re.split(r'(?<=[.!?])\\s+', text)  # Tách câu dựa trên dấu câu\n    seen = set()\n    filtered_sentences = []\n    \n    for sentence in sentences:\n        if sentence not in seen:  # Chỉ thêm câu nếu nó chưa xuất hiện trước đó\n            filtered_sentences.append(sentence)\n            seen.add(sentence)\n    \n    return \" \".join(filtered_sentences)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def inference(tokenizer, model, question, choices, generation_config, device=\"cpu\"):\n    prompt = make_ans_prompt(question, choices)\n    encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    with torch.inference_mode():\n        outputs = model.generate(\n            input_ids=encoding.input_ids,\n            attention_mask=encoding.attention_mask,\n            generation_config=generation_config\n        )\n    ans = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    processed_ans = remove_duplicate_sentences(ans)\n    processed_ans = processed_ans.split(\"<|im_start|> assistant\")\n    return processed_ans[1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T16:11:26.101639Z","iopub.execute_input":"2025-02-21T16:11:26.101960Z","iopub.status.idle":"2025-02-21T16:11:26.106747Z","shell.execute_reply.started":"2025-02-21T16:11:26.101934Z","shell.execute_reply":"2025-02-21T16:11:26.105878Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"question = \"\"\"\nBella đã mua tem tại bưu điện. Một số tem có thiết kế tuyết rơi, một số có thiết kế xe tải và một số có thiết kế hoa hồng. Bella đã mua 11 tem tuyết rơi. Cô ấy đã mua 9 tem xe tải nhiều hơn số tem tuyết rơi và ít hơn 13 tem hoa hồng so với số tem xe tải. Bella đã mua tổng cộng bao nhiêu tem?\n\"\"\".strip()\n\nchoices = \"\"\"\"\"\"\n\nchoices = choices if choices != \"\"\"\"\"\" else None\n\nprint(question)\nprint(choices)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T16:29:34.444592Z","iopub.execute_input":"2025-02-21T16:29:34.444908Z","iopub.status.idle":"2025-02-21T16:29:34.450046Z","shell.execute_reply.started":"2025-02-21T16:29:34.444885Z","shell.execute_reply":"2025-02-21T16:29:34.449196Z"}},"outputs":[{"name":"stdout","text":"Bella đã mua tem tại bưu điện. Một số tem có thiết kế tuyết rơi, một số có thiết kế xe tải và một số có thiết kế hoa hồng. Bella đã mua 11 tem tuyết rơi. Cô ấy đã mua 9 tem xe tải nhiều hơn số tem tuyết rơi và ít hơn 13 tem hoa hồng so với số tem xe tải. Bella đã mua tổng cộng bao nhiêu tem?\nNone\n","output_type":"stream"}],"execution_count":72},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nans = inference(tokenizer, model, question, choices, generation_config, device=\"cuda\")\nprint(ans)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T16:29:36.704339Z","iopub.execute_input":"2025-02-21T16:29:36.704711Z","iopub.status.idle":"2025-02-21T16:30:02.582654Z","shell.execute_reply.started":"2025-02-21T16:29:36.704680Z","shell.execute_reply":"2025-02-21T16:30:02.581847Z"}},"outputs":[{"name":"stdout","text":"\nBella đã mua 20 tem xe tải (11 + 9). Cô ấy đã mua 7 tem hoa hồng (20 - 13). Bella đã mua tổng cộng 38 tem (11 + 20 + 7). Bella đã mua tổng cộng 38 tem. Câu trả lời là: 38 tem. Đáp án là 38\n\nChọn đáp án đúng\nBella đã mua tổng cộng 38 tem. Đáp án là 38\n\nChọn đáp án đúng\nBella đã mua\n","output_type":"stream"}],"execution_count":73},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}