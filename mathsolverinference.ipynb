{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":223675230,"sourceType":"kernelVersion"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U bitsandbytes\n!pip install -q -U datasets\n!pip install -q -U git+https://github.com/huggingface/transformers.git\n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install -q -U git+https://github.com/huggingface/accelerate.git\n!pip install -q -U loralib\n!pip install -q -U einops","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T15:20:31.739370Z","iopub.execute_input":"2025-02-21T15:20:31.739682Z","iopub.status.idle":"2025-02-21T15:21:46.066118Z","shell.execute_reply.started":"2025-02-21T15:20:31.739650Z","shell.execute_reply":"2025-02-21T15:21:46.065040Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for accelerate (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport torch\n# from constant import (\n#     MODEL_NAME, R, LORA_ALPHA, TARGET_MODULES, LORA_DROPOUT, BIAS, TASK_TYPE, \n#     MAX_NEW_TOKENS, TEMPERATURE, TOP_P, NUM_RETURN_SEQUENCES\n# )\nfrom transformers import (\n    AutoConfig, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n)\nfrom peft import (\n    LoraConfig, PeftConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n# MODEL GENERATE CONFIG\nMAX_NEW_TOKENS=200 # max length of generated tokens\nTEMPERATURE=0.7 # controls randomness in generation\nTOP_P=0.7 # nucleus sampling parameter\nNUM_RETURN_SEQUENCES=1 # number of generated sequences to return\n\n\ndef get_tokenizer():\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    tokenizer.pad_token = tokenizer.eos_token\n    return tokenizer\n\ndef print_trainable_parameters(model):\n    trainable_params = 0\n    all_param = 0\n\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainables%: {100 * trainable_params / all_param}\"\n    )\n\ndef get_generate_config(tokenizer, model):\n    generation_config = model.generation_config\n    generation_config.max_new_tokens = MAX_NEW_TOKENS\n    generation_config.temperature = TEMPERATURE\n    generation_config.top_p = TOP_P\n    generation_config.num_return_sequences = NUM_RETURN_SEQUENCES\n    generation_config.pad_token_id = tokenizer.eos_token_id\n    generation_config.eos_token_id = tokenizer.eos_token_id\n\n    return generation_config\n\nif __name__ == \"__main__\":\n    pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T16:43:58.989430Z","iopub.execute_input":"2025-02-21T16:43:58.989797Z","iopub.status.idle":"2025-02-21T16:43:58.998620Z","shell.execute_reply.started":"2025-02-21T16:43:58.989768Z","shell.execute_reply":"2025-02-21T16:43:58.997930Z"}},"outputs":[],"execution_count":78},{"cell_type":"code","source":"import torch\nimport re\n# from config_model import (\n#     get_tokenizer,\n#     get_generate_config,\n#     bnb_config,\n#     lora_config\n# )\n# from constant import PROMTP_ANS_FORMAT, FINETUNED_MODEL\n# from config_model import bnb_config\nfrom transformers import (\n    AutoConfig, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n)\nfrom peft import (\n    LoraConfig, PeftConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n)\n\nPROMTP_ANS_FORMAT = \"\"\"\n<|im_start|>system\n{}\n<|im_end|>\n<|im_start|>user\n{}\n<|im_end|>\n<|im_start|>assistant\n\"\"\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\ndef make_ans_prompt(question, choices=None):\n    if choices is not None:\n        in_context_key = \"2\"\n        choices = f\"\"\"### Các lựa chọn\\n{choices}\n        \"\"\"\n    else:\n        in_context_key = \"1\"\n        choices = \"\"\n    \n    instruction = question + \"\\n\" + choices\n    instruction = instruction.strip()\n    \n    prompt = PROMTP_ANS_FORMAT.format(\n        IN_CONTEXT_PROMPT[in_context_key],\n        instruction\n    )\n\n    return prompt\n\ndef remove_duplicate_sentences(text):\n    sentences = re.split(r'(?<=[.!?])\\s+', text)  # Tách câu dựa trên dấu câu\n    seen = set()\n    filtered_sentences = []\n    \n    for sentence in sentences:\n        if sentence not in seen:  # Chỉ thêm câu nếu nó chưa xuất hiện trước đó\n            filtered_sentences.append(sentence)\n            seen.add(sentence)\n    \n    return \" \".join(filtered_sentences)\n\n\ndef inference(tokenizer, model, question, choices, generation_config, device=\"cpu\"):\n    prompt = make_ans_prompt(question, choices)\n    encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    with torch.inference_mode():\n        outputs = model.generate(\n            input_ids=encoding.input_ids,\n            attention_mask=encoding.attention_mask,\n            generation_config=generation_config\n        )\n    ans = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    processed_ans = remove_duplicate_sentences(ans)\n    processed_ans = processed_ans.split(\"<|im_start|> assistant\")\n    return processed_ans[1]\n\ndef make_inference():\n    # USER INPUT\n\n    question = input(\"Input your question: \").strip()\n\n    choices = input(\"Choices (Optional): \").strip()\n    choices = choices if choices != \"\" else None\n\n    print(\"Generating Answer...\")\n    ans = inference(tokenizer, model, question, choices, generation_config, device=device)\n    print(ans)\n\nif __name__ == \"__main__\":\n    \n    # LOAD MODEL\n    print(\"Loading Model\")\n    config = PeftConfig.from_pretrained(FINETUNED_MODEL)\n\n    model = AutoModelForCausalLM.from_pretrained(\n        config.base_model_name_or_path,\n        return_dict=True,\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n        trust_remote_code=True\n    )\n\n    tokenizer=AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n    tokenizer.pad_token = tokenizer.eos_token\n\n    model = PeftModel.from_pretrained(model, FINETUNED_MODEL)\n\n    generation_config = get_generate_config(tokenizer, model)\n\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T16:49:46.577917Z","iopub.execute_input":"2025-02-21T16:49:46.578254Z","iopub.status.idle":"2025-02-21T16:50:03.553125Z","shell.execute_reply.started":"2025-02-21T16:49:46.578226Z","shell.execute_reply":"2025-02-21T16:50:03.552403Z"}},"outputs":[{"name":"stdout","text":"Loading Model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2664dc939728496f8acdfe60a59750d1"}},"metadata":{}}],"execution_count":82},{"cell_type":"code","source":"make_inference()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T16:50:03.554412Z","iopub.execute_input":"2025-02-21T16:50:03.554816Z","iopub.status.idle":"2025-02-21T16:50:34.496695Z","shell.execute_reply.started":"2025-02-21T16:50:03.554776Z","shell.execute_reply":"2025-02-21T16:50:34.495630Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Input your question:  Natalia đã bán kẹp tóc cho 48 người bạn của cô ấy vào tháng 4, và sau đó cô ấy đã bán nửa số lượng kẹp tóc đó vào tháng 5. Natalia đã bán tổng cộng bao nhiêu kẹp tóc trong tháng 4 và tháng 5?\nChoices (Optional):  \n"},{"name":"stdout","text":"Generating Answer...\n\nNatalia đã bán được 48 / 2 = 24 cái kẹp tóc trong tháng 5. Tổng cộng, cô ấy đã bán được 48 + 24 = 72 cái kẹp tóc. Natalia đã bán được 72 cái kẹp tóc trong tháng 4 và tháng 5. N\n","output_type":"stream"}],"execution_count":83},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}